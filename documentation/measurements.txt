iterations:                 2000
epsilon:                    0.2 (no random bombs)

alpha:                      1e-3, 5e-4, 1e-4, 5e-5                          (default: 1e-4)         Rudolf
gamma:                      0.9, 0.8, 0.7, 0.6                              (default: 0.8)          Maria
batch size, buffer size:    (25, 50), (50, 100), (250, 500), (500,1000)     (default: (50, 100))    Jannik

python main.py play --agents ml_agent_1_m? --scenario coin-heaven --train 1 --n-rounds 2000 --no-gui


iterations:                 10000
epsilon (start, decrease, min): 1.0 / 0.9995 / 0.1

alpha:                      1e-3, 5e-4, 1e-4, 5e-5                          (default: 1e-4)         
gamma:                      0.9, 0.8, 0.7, 0.6, 0.5                         (default: 0.6)          
batch size, buffer size:    50

Maria:  1e-3 mit allen gammas
Rudolf: 5e-5 mit allen gammas
Jannik: 1e-4 mit allen gammas
        5e-4

python main.py play --agents ml_agent_1_m? --scenario crate-heaven --train 1 --n-rounds 10000 --no-gui

----------------------------------
Measurements for report:

For all graphs: Rolling mean with n=500

✓✓ Hyperparameter evaluation:

        Alpha: 0.001, 0.0001, 0.00001
        Gamma: 0.6, 0.5, 0.4
        --> all combinations

        Epsilon Start: 1.0
        Epsilon Min: 0.1 (set to 0 after this)
        Epsilon decrease: 0.99967

        Episodes: 10000 (Epsilon is 0 after ~7000 episodes)

        Scenario: Classic
        Opponents: None
        Length of the game: 200

h1 0 0
h2 0 1
h3 0 2
h4 1 0
h5 1 1
h6 1 2
h7 2 0
h8 2 1
h9 2 2	

python main.py play --agent solid_snake_h --scenario classic --train 1 --n-rounds 10000 --no-gui

-------------------------------------------------

For all following trainings:

        Epsilon Start: 1.0
        Epsilon Min: 0.1 (set to 0 after this)
        Epsilon decrease: 0.99967

        Episodes: 10000 (Epsilon is 0 after ~7000 episodes)

        Scenario: Classic
        Opponents: one rule_based_agent
        Length of the game: 400

	python main.py play --agent ? rule_based_agent --scenario classic --train 1 --n-rounds 10000 --no-gui

For all following test:

        Episodes: 1000
        Opponents: one rule_based_agent

	python main.py play --agent ? rule_based_agent --scenario classic --n-rounds 1000 --no-gui --save-stats results/for_report/[filename].json

solid_snake:
        - ✓✓ training for intermediate steps (--> training process)
        - ✓✓ training from zero to compare solid_snake to the ensemble agents
        - ✓✓ testing the trained from zero model 
	- ✓✓ testing for intermediate steps
        - ✓/ possibly more training/testing with more rule_based_agents or self training

ensemble/nstep training:
        - ✓✓ evaluate decision modes with optimal Hyperparameters and different n (N = 1 ... 20) + with optimal Hyperparameters and n (N = 0,0,0,0,1,5,10,20) --> [ens 1,3,4 + 2,5,6] (modes 1, 0, 2)
        - ✓✓ train single n-step Q-learning models and compare different n (N = 1, 5, 10, 20) + compare to normal Q-learning (already trained before) --> [q1 - q4]
        - ✓✓ what happens if we train the ensemble with different Hyperparameters (and optimal decision mode)? --> [ens 7]

ensemble/nstep testing:
        - ✓✓ try to use models from Hyperparameter evaluation in ensemble (possibly, higher weight for the better models) [ens_9]
        - ✓✓ performance test for q-learning models
	- ✓✓ performance test for decision mode evaluation models (ens 1-6)
	- ✓✓ performance test for ens 7

✓✓ what happens if gradient update in every step? [solid_snake_stepupdate]

✓✓ train ensemble with N=0 --> possibly compare to solid_snake [ens_8]
✓✓ test ensemble with N=0

possibly, continue training the best ensemble